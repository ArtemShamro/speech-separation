defaults:
  - model: dttnet
  - writer: cometml
  - metrics: example
  - datasets: train
  - dataloader: baseline
  - transforms: baseline
  - _self_
model_loader:
  # patterns:
  # no pretrained
  # "no"
  # from writer (implemented for comet-ml):
  # "writer:{experiment_id}"
  # from huggingface:
  # accout/repo@branch - expecting file model.pth exists
  # from local model
  # "{full path to .pth file with ctexkpoint or state_dict}"
  from_pretrained: "no" # "writer:14rdyr4kpmmhoq1q40qlhbegjspdhqco" # "/home/atem/HSE_videos/4_DLA/other/model.pth" # "areshx/test_repo@main" # "areshx/test@main"
optimizer:
  _target_: torch.optim.AdamW
  lr: 1e-3
  weight_decay: 1e-5
lr_scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: [1e-3, 1e-3]
  final_div_factor: 1e2
  pct_start: 0.1
  steps_per_epoch: ${trainer.epoch_len}
  epochs: ${trainer.n_epochs}
  anneal_strategy: cos
loss_function:
  _target_: src.loss.PITLossWrapper
  loss_function:
    _target_: src.loss.DTTNetLoss
trainer:
  use_profiler: False
  log_step: 100
  n_epochs: 100
  epoch_len: null
  device_tensors: ["spectrogram", "phase", "sources", "audio", "audio_length", "video"] # which tensors should be on device (ex. GPU)
  # patterns:
  # no resume
  # "no" or null
  # from local checkpoint
  # "{checkpoint path}"
  # from writer best_model.pth artifact (implemented for comet-ml)
  # "{experiment_id}""
  resume_from: null # "14rdyr4kpmmhoq1q40qlhbegjspdhqco" # "/home/atem/HSE_videos/4_DLA/other/model_best.pth" # null or path to the checkpoint dir with *.pth and config.yaml
  device: auto # device name or "auto"
  override: True # if True, will override the previous run with the same name
  monitor: "max val_SI-SNRi" # "off" or "max/min metric_name", i.e. our goal is to maximize/minimize metric
  save_period: 10 # checkpoint each save_period epochs in addition to the best epoch
  early_stop: 100 # epochs for early stopping
  save_dir: "saved"
  seed: 1
  hugging_face_repo: "areshx/test_repo@main" # "repo@branch"
  train_log_items: 10 # n_items to log wav and wav_aug each train logtime
  # max_grad_norm: 1
